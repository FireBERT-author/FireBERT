{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 FireBERT authors. All rights reserved.\n",
    "#\n",
    "# Licensed under the MIT license\n",
    "# See https://github.com/FireBERT-author/FireBERT/blob/master/LICENSE for details\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from processors import MnliProcessor\n",
    "from bert_base_model import LightningBertForSequenceClassification\n",
    "from firebert_fve import FireBERT_FVE\n",
    "from firebert_fse import FireBERT_FSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base MNLI model from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the base model tuned on MNLI\n",
    "model_dir = 'resources/models/MNLI/pytorch_model.bin'\n",
    "\n",
    "# prepare hyperparameters\n",
    "hparams = {'batch_size': 32 }\n",
    "\n",
    "# instantiate the model\n",
    "model = LightningBertForSequenceClassification(load_from=model_dir, \n",
    "                                               processor=MnliProcessor(), \n",
    "                                               hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848ed5cec2214cffb41f41c7000215cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=307.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.8398), 'avg_test_f1': tensor(0.8331, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.8397846221923828, 'avg_test_f1': 0.8331295236055348}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the metrics for the model against validation data\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='dev')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35afe7b381ac477d909b80c5d55201b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=308.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.8425), 'avg_test_f1': tensor(0.8352, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.8425324559211731, 'avg_test_f1': 0.8351874185857123}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test set\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='test')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11bcde00d9f47aba71a9bd6ca7df5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=235.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.0371), 'avg_test_f1': tensor(0.0355, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.03710106387734413, 'avg_test_f1': 0.035549824956334554}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare how well the base model does against adversarial dev samples\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_dev')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d840e2440345b889ab001584a5f566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=237.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.0287), 'avg_test_f1': tensor(0.0273, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.02874472551047802, 'avg_test_f1': 0.027315982757506095}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the adversarial test set\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_test')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI model tuned on lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our version of the model tuned on the MNLI task\n",
    "model_dir = 'resources/models/MNLI_on_lightning/pytorch_model.bin'\n",
    "\n",
    "# prepare hyperparameters\n",
    "hparams = {'batch_size': 32 }\n",
    "\n",
    "# instantiate the model\n",
    "model = LightningBertForSequenceClassification(load_from=model_dir, \n",
    "                                               processor=MnliProcessor(), \n",
    "                                               hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f84369298644063a6bf011387f6976c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=307.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.8279), 'avg_test_f1': tensor(0.8211, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.8278528451919556, 'avg_test_f1': 0.821133296087285}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the metrics for the model against validation data\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='dev')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c4fca2f1c149739e64f58d9378bb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=308.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.8330), 'avg_test_f1': tensor(0.8259, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.8329951167106628, 'avg_test_f1': 0.8259453206441073}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test set\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='test')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea7f73f184c402f8286f22e6bb2a63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=235.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.4900), 'avg_test_f1': tensor(0.4733, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.490026593208313, 'avg_test_f1': 0.47326726672573016}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare how well the model does against adversarial dev samples\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_dev')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb6ffbd56584591a96f0b750dcc23d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=237.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.5012), 'avg_test_f1': tensor(0.4867, dtype=torch.float64)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.5011747479438782, 'avg_test_f1': 0.48670233734742874}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the adversarial test set\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_test')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIVE on base MNLI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our co-tuned model\n",
    "model_dir = 'resources/models/MNLI/pytorch_model.bin'\n",
    "\n",
    "# prepare hyperparameters\n",
    "hparams =  {'batch_size': 8, 'use_USE': False, 'stop_words': True, 'perturb_words': 1, \n",
    "            'verbose': False, 'vote_avg_logits': True, 'std': 8.139999999999995, 'vector_count': 8}\n",
    "\n",
    "# instantiate the model\n",
    "model = FireBERT_FVE(load_from=model_dir, \n",
    "                     processor=MnliProcessor(), \n",
    "                     hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=1227.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.7479), 'avg_test_f1': tensor(0.6972)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.7479188442230225, 'avg_test_f1': 0.6971950531005859}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the metrics for the model against validation data\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='dev')\n",
    "model.set_test_dataset(dataset, examples=examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71f3834ddce445a907e154e5e84865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=1229.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.7563), 'avg_test_f1': tensor(0.7081)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.7563059329986572, 'avg_test_f1': 0.7081065773963928}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test set\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='test')\n",
    "model.set_test_dataset(dataset, examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a6db9075ea42beb93885863fb42c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=937.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.6029), 'avg_test_f1': tensor(0.5400)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.60285484790802, 'avg_test_f1': 0.5400167107582092}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare how well the model does against adversarial dev samples\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_dev')\n",
    "model.set_test_dataset(dataset, examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc46f2672b7d42c785dce89f501a9e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=947.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.6321), 'avg_test_f1': tensor(0.5664)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.6320837736129761, 'avg_test_f1': 0.5663667917251587}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the adversarial test set\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_test')\n",
    "model.set_test_dataset(dataset, examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FuSE on base MNLI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using scratch/tf_cache to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# load our co-tuned model\n",
    "model_dir = 'resources/models/MNLI/pytorch_model.bin'\n",
    "\n",
    "# prepare hyperparameters\n",
    "hparams =  {'use_USE':True, 'USE_method':\"filter\", 'USE_multiplier':20, 'stop_words':True, 'perturb_words':2,\n",
    "            'candidates_per_word':8, 'total_alternatives':14, 'match_pos':True, 'batch_size':1,'verbose':False, \n",
    "            'vote_avg_logits':True}\n",
    "\n",
    "# instantiate the model\n",
    "model = FireBERT_FSE(load_from=model_dir, \n",
    "                     processor=MnliProcessor(), \n",
    "                     hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702f92e5f5504ee585948eeb9563d234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=9815.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.7145), 'avg_test_f1': tensor(0.7145)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.7145186066627502, 'avg_test_f1': 0.7145186066627502}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the metrics for the model against validation data\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='dev')\n",
    "model.set_test_dataset(dataset, examples=examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b14d5c26534e0e91032b14170c256b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=9832.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.7250), 'avg_test_f1': tensor(0.7250)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.7249796390533447, 'avg_test_f1': 0.7249796390533447}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test set\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='test')\n",
    "model.set_test_dataset(dataset, examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ed1a8d8a13461e90ef5d158e637ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=7490.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.5893), 'avg_test_f1': tensor(0.5893)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.589319109916687, 'avg_test_f1': 0.589319109916687}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare how well the model does against adversarial dev samples\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_dev')\n",
    "model.set_test_dataset(dataset, examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f6a6891010467ebfabcca6be5c46e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=7574.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.5939), 'avg_test_f1': tensor(0.5939)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.5938737988471985, 'avg_test_f1': 0.5938737988471985}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the adversarial test set\n",
    "dataset, examples = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_test')\n",
    "model.set_test_dataset(dataset, examples)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-tuned MNLI model from FACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our co-tuned model\n",
    "model_dir = 'resources/models/co-tuned_MNLI_on_lightning_final_filter/pytorch_model.bin'\n",
    "\n",
    "# prepare hyperparameters\n",
    "hparams = {'batch_size': 32 }\n",
    "\n",
    "# instantiate the model\n",
    "model = LightningBertForSequenceClassification(load_from=model_dir, \n",
    "                                               processor=MnliProcessor(), \n",
    "                                               hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438e727db5844264a0c94cef03585aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=307.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.8240), 'avg_test_f1': tensor(0.8167)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.8240068554878235, 'avg_test_f1': 0.8167411088943481}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the metrics for the model against validation data\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='dev')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ede5fed73d44d1389d7971619cca92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=308.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.8273), 'avg_test_f1': tensor(0.8205)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.8273133039474487, 'avg_test_f1': 0.8205042481422424}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test set\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='test')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1059b0396e3a43f493dee619ce7ef8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=235.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.7846), 'avg_test_f1': tensor(0.7754)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.7845744490623474, 'avg_test_f1': 0.7753534913063049}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare how well the model does against adversarial dev samples\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_dev')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023f134258d94aa59bb145ea47c6ebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=237.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_acc': tensor(0.8003), 'avg_test_f1': tensor(0.7908)}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_test_acc': 0.8003212213516235, 'avg_test_f1': 0.7907801270484924}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the adversarial test set\n",
    "dataset, _ = model.get_processor().load_and_cache_examples(\"data/MNLI\", example_set='adv_test')\n",
    "model.set_test_dataset(dataset)\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.test(model)\n",
    "trainer.tqdm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
